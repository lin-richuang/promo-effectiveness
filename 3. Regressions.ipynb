{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression \n",
    "\n",
    "This notebook is mainly for basic regressions and evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import neighbors\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model as KerasModel\n",
    "from keras.layers import Input, Dense, Activation, Reshape, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "SHUFFLE_DATA = True\n",
    "ONE_HOT_AS_INPUT = False # in addition to state holidays, use one hot for other categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_dump = 'models/xgb_model.pickle'\n",
    "rf_model_dump = 'models/rf_model.pickle'\n",
    "nn_model_dump = 'models/nn_model.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = 'processed/'\n",
    "processed_file = 'processed_data.csv'\n",
    "X = pd.read_csv(processed_path + processed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = X.pop('Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using one-hot encoding as input for model\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Store', 'DayOfWeek', 'weekofyear'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-3ee468d25b4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using one-hot encoding as input for model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcols_for_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Store'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DayOfWeek'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weekofyear'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mone_hot_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_for_one_hot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols_for_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/embedding/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/embedding/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         )\n\u001b[1;32m   1555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/embedding/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1638\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1640\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['Store', 'DayOfWeek', 'weekofyear'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "if ONE_HOT_AS_INPUT:\n",
    "    print(\"Using one-hot encoding as input for model\")\n",
    "    cols_for_one_hot = ['Store', 'DayOfWeek', 'weekofyear']\n",
    "    one_hot_df = X[cols_for_one_hot]\n",
    "    X = X.drop(columns = cols_for_one_hot)\n",
    "    \n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    enc.fit(one_hot_df)\n",
    "    one_hot_df = enc.transform(one_hot_df)\n",
    "    X = X.to_numpy()\n",
    "    X = pd.concat([X, one_hot_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use last 10% data as validation data\n",
    "num_record = len(X)\n",
    "train_size = int(TRAIN_RATIO * num_record)\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_val = X[train_size:]\n",
    "y_val = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using shuffled data\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-42f244cd073c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "if SHUFFLE_DATA:\n",
    "    print(\"Using shuffled data\")\n",
    "    sh = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(sh)\n",
    "    X_train = X_train.iloc[sh]\n",
    "    y_train = y_train.iloc[sh]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def evaluate(self, X_val, y_val):\n",
    "        assert (min(y_val) > 0)\n",
    "        guessed_sales = self.guess(X_val)\n",
    "        relative_err = np.absolute((y_val - guessed_sales) / y_val)\n",
    "        result = np.sum(relative_err) / len(y_val)\n",
    "        return result\n",
    "\n",
    "    def rsqr_score(self, X_val, y_val):\n",
    "        assert (min(y_val) > 0)\n",
    "        guessed_sales = self.guess(X_val)\n",
    "        SS_Residual = sum((y_val-guessed_sales)**2)\n",
    "        SS_Total = sum((y_val-np.mean(y_val))**2)\n",
    "        r_squared = 1 - (float(SS_Residual))/SS_Total\n",
    "        adjusted_r_squared = 1 - (1-r_squared)*(len(y_val)-1)/(len(y_val)-X_val.shape[1]-1)\n",
    "        return adjusted_r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(Model):\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.clf = linear_model.LinearRegression()\n",
    "        self.clf.fit(X_train, np.log(y_train))\n",
    "        print(\"Result on validation set is: \", self.evaluate(X_val, y_val))\n",
    "        print(\"R squared on validation set is: \", self.rsqr_score(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        return np.exp(self.clf.predict(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression(Model):\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.clf = linear_model.Ridge(alpha=1.0)\n",
    "        self.clf.fit(X_train, np.log(y_train))\n",
    "        print(\"MAPE on validation set is: \", self.evaluate(X_val, y_val))\n",
    "        print(\"R squared on validation set is: \", self.rsqr_score(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        return np.exp(self.clf.predict(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RF(Model):\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.clf = RandomForestRegressor(n_estimators=200,\n",
    "                                         verbose=True,\n",
    "                                         max_depth=35,\n",
    "                                         min_samples_split=2,\n",
    "                                         min_samples_leaf=1\n",
    "                                         )\n",
    "        self.clf.fit(X_train, np.log(y_train))\n",
    "        print(\"MAPE on validation set is: \", self.evaluate(X_val, y_val))\n",
    "        print(\"R squared on validation set is: \", self.rsqr_score(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        return np.exp(self.clf.predict(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.__normalize_data()\n",
    "        self.clf = SVR(kernel='linear', degree=3, gamma='auto', coef0=0.0, tol=0.001,\n",
    "                       C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
    "\n",
    "        self.clf.fit(self.X_train, np.log(self.y_train))\n",
    "        print(\"MAPE on validation set is: \", self.evaluate(X_val, y_val))\n",
    "        print(\"R squared on validation set is: \", self.rsqr_score(X_val, y_val))\n",
    "\n",
    "    def __normalize_data(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
    "\n",
    "    def guess(self, feature):\n",
    "        return np.exp(self.clf.predict(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGB(Model):\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        dtrain = xgb.DMatrix(X_train, label=np.log(y_train))\n",
    "        evallist = [(dtrain, 'train')]\n",
    "        params = {'nthread': -1,\n",
    "                  'max_depth': 7,\n",
    "                  'eta': 0.02,\n",
    "                  'silent': 1,\n",
    "                  'objective': 'reg:squarederror',\n",
    "                  'colsample_bytree': 0.7,\n",
    "                  'subsample': 0.7}\n",
    "        num_round = 3000\n",
    "        self.bst = xgb.train(params,\n",
    "                             dtrain,\n",
    "                             num_round,\n",
    "                             evallist,\n",
    "                             verbose_eval = 50,\n",
    "                             early_stopping_rounds = 50)\n",
    "        print(\"MAPE on validation set is: \", self.evaluate(X_val, y_val))\n",
    "        print(\"R squared on validation set is: \", self.rsqr_score(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        dtest = xgb.DMatrix(features)\n",
    "        return np.exp(self.bst.predict(dtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.normalizer = Normalizer()\n",
    "        self.normalizer.fit(X_train)\n",
    "        self.clf = neighbors.KNeighborsRegressor(n_neighbors=10, weights='distance', p=1)\n",
    "        self.clf.fit(self.normalizer.transform(X_train), np.log(y_train))\n",
    "        print(\"MAPE on validation set is: \", self.evaluate(self.normalizer.transform(X_val), y_val))\n",
    "        print(\"R squared on validation set is: \", self.rsqr_score(self.normalizer.transform(X_val), y_val))\n",
    "\n",
    "    def guess(self, feature):\n",
    "        return np.exp(self.clf.predict(self.normalizer.transform(feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.epochs = 10\n",
    "        self.checkpointer = ModelCheckpoint(filepath=\"models/best_model_weights.hdf5\", verbose=1, save_best_only=True)\n",
    "        self.max_log_y = max(np.max(np.log(y_train)), np.max(np.log(y_val)))\n",
    "        self.__build_keras_model()\n",
    "        self.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    def __build_keras_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(1000, kernel_initializer=\"uniform\", input_dim=1181))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(500, kernel_initializer=\"uniform\"))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.add(Activation('sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "    def _val_for_fit(self, val):\n",
    "        val = np.log(val) / self.max_log_y\n",
    "        return val\n",
    "\n",
    "    def _val_for_pred(self, val):\n",
    "        return np.exp(val * self.max_log_y)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        self.model.fit(X_train, self._val_for_fit(y_train),\n",
    "                       validation_data=(X_val, self._val_for_fit(y_val)),\n",
    "                       epochs=self.epochs, batch_size=128,\n",
    "                       # callbacks=[self.checkpointer],\n",
    "                       )\n",
    "        # self.model.load_weights('models/best_model_weights.hdf5')\n",
    "        print(\"MAPE on validation set is: \", self.evaluate(X_val, y_val))\n",
    "        print(\"R squared on validation set is: \", self.rsqr_score(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        result = self.model.predict(features).flatten()\n",
    "        return self._val_for_pred(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Linear Model...\n",
      "Result on validation set is:  0.3025476419720422\n",
      "R squared on validation set is:  0.12065762984475714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LinearModel at 0x7fa760972410>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Fitting Linear Model...\")\n",
    "LinearModel(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Ridge Model...\n",
      "MAPE on validation set is:  0.14558901746469502\n",
      "R squared on validation set is:  0.7905570253579708\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting Ridge Model...\")\n",
    "ridge_model = RidgeRegression(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting KNN...\n",
      "MAPE on validation set is:  0.19840082150007332\n",
      "R squared on validation set is:  0.5062692270813305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.KNN at 0x7fa7207bff50>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Fitting KNN...\")\n",
    "KNN(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SVM...\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting SVM...\")\n",
    "SVM(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Random Forest Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:  7.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE on validation set is:  0.15194456273543347\n",
      "R squared on validation set is:  0.7049265011524837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    2.9s finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting Random Forest Model...\")\n",
    "rf_model = RF(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting XGB Model...\n",
      "[11:23:06] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:8.09899\n",
      "Will train until train-rmse hasn't improved in 50 rounds.\n",
      "[50]\ttrain-rmse:2.97030\n",
      "[100]\ttrain-rmse:1.13397\n",
      "[150]\ttrain-rmse:0.53058\n",
      "[200]\ttrain-rmse:0.37779\n",
      "[250]\ttrain-rmse:0.34390\n",
      "[300]\ttrain-rmse:0.33204\n",
      "[350]\ttrain-rmse:0.32451\n",
      "[400]\ttrain-rmse:0.31565\n",
      "[450]\ttrain-rmse:0.30715\n",
      "[500]\ttrain-rmse:0.29979\n",
      "[550]\ttrain-rmse:0.29390\n",
      "[600]\ttrain-rmse:0.28630\n",
      "[650]\ttrain-rmse:0.28104\n",
      "[700]\ttrain-rmse:0.27486\n",
      "[750]\ttrain-rmse:0.26927\n",
      "[800]\ttrain-rmse:0.26436\n",
      "[850]\ttrain-rmse:0.26021\n",
      "[900]\ttrain-rmse:0.25626\n",
      "[950]\ttrain-rmse:0.25089\n",
      "[1000]\ttrain-rmse:0.24656\n",
      "[1050]\ttrain-rmse:0.24272\n",
      "[1100]\ttrain-rmse:0.23930\n",
      "[1150]\ttrain-rmse:0.23561\n",
      "[1200]\ttrain-rmse:0.23224\n",
      "[1250]\ttrain-rmse:0.22945\n",
      "[1300]\ttrain-rmse:0.22651\n",
      "[1350]\ttrain-rmse:0.22312\n",
      "[1400]\ttrain-rmse:0.22015\n",
      "[1450]\ttrain-rmse:0.21698\n",
      "[1500]\ttrain-rmse:0.21399\n",
      "[1550]\ttrain-rmse:0.21145\n",
      "[1600]\ttrain-rmse:0.20894\n",
      "[1650]\ttrain-rmse:0.20686\n",
      "[1700]\ttrain-rmse:0.20514\n",
      "[1750]\ttrain-rmse:0.20285\n",
      "[1800]\ttrain-rmse:0.20055\n",
      "[1850]\ttrain-rmse:0.19864\n",
      "[1900]\ttrain-rmse:0.19624\n",
      "[1950]\ttrain-rmse:0.19384\n",
      "[2000]\ttrain-rmse:0.19179\n",
      "[2050]\ttrain-rmse:0.18984\n",
      "[2100]\ttrain-rmse:0.18793\n",
      "[2150]\ttrain-rmse:0.18594\n",
      "[2200]\ttrain-rmse:0.18416\n",
      "[2250]\ttrain-rmse:0.18249\n",
      "[2300]\ttrain-rmse:0.18032\n",
      "[2350]\ttrain-rmse:0.17881\n",
      "[2400]\ttrain-rmse:0.17737\n",
      "[2450]\ttrain-rmse:0.17564\n",
      "[2500]\ttrain-rmse:0.17435\n",
      "[2550]\ttrain-rmse:0.17302\n",
      "[2600]\ttrain-rmse:0.17133\n",
      "[2650]\ttrain-rmse:0.16989\n",
      "[2700]\ttrain-rmse:0.16821\n",
      "[2750]\ttrain-rmse:0.16678\n",
      "[2800]\ttrain-rmse:0.16550\n",
      "[2850]\ttrain-rmse:0.16425\n",
      "[2900]\ttrain-rmse:0.16266\n",
      "[2950]\ttrain-rmse:0.16114\n",
      "[2999]\ttrain-rmse:0.16007\n",
      "MAPE on validation set is:  0.16469142930779596\n",
      "R squared on validation set is:  0.794093942874833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.XGB at 0x7fa7533bd1d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Fitting XGB Model...\")\n",
    "xgb_model = XGB(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting NN...\n",
      "Train on 759904 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "759904/759904 [==============================] - 121s 159us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "Epoch 2/10\n",
      "759904/759904 [==============================] - 113s 149us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "Epoch 3/10\n",
      "759904/759904 [==============================] - 105s 139us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "Epoch 4/10\n",
      "759904/759904 [==============================] - 104s 136us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "Epoch 5/10\n",
      "759904/759904 [==============================] - 105s 138us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "Epoch 6/10\n",
      "759904/759904 [==============================] - 105s 138us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "Epoch 7/10\n",
      "759904/759904 [==============================] - 105s 138us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "Epoch 8/10\n",
      "759904/759904 [==============================] - 105s 138us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "Epoch 9/10\n",
      "759904/759904 [==============================] - 102s 134us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "Epoch 10/10\n",
      "759904/759904 [==============================] - 92s 122us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "MAPE on validation set is:  5.826428694123841\n",
      "R squared on validation set is:  -127.27416796877921\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting NN...\")\n",
    "nn_model = NN(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rf_model_dump, 'wb') as f:\n",
    "    pickle.dump(rf_model, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(xgb_model_dump, 'wb') as f:\n",
    "    pickle.dump(xgb_model, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(nn_model_dump, 'wb') as f:\n",
    "    pickle.dump(nn_model, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
